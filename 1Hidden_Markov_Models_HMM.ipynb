{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIsKYjTJaXN0PVwKAKLNbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescopatane96/Hidden_Markov_Models/blob/main/1Hidden_Markov_Models_HMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hidden Markov Models"
      ],
      "metadata": {
        "id": "YQ8LSbJ1mlep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM are probabilistic models used for structured predictions. they are popular for tagging all elements in a sequence (in the space or in the time) with some hidden states. in a hidden markov models there are 2 types of data: \n",
        "\n",
        "- States: ex. [cloud, rain, sun] or [in, out, transmembrane]. they are called 'hidden' because we don't see them.\n",
        "\n",
        "- Observations: ex. [monday, tuesday, wednesday] or [A, Y, M] where A,Y,M are aminoacids incorporate in the protein sequence.\n",
        "\n",
        "hidden states can be thought of as exstensions of so called Markov chains where the probability of the next hidden state is dependant on the current (t-1) hidden state, and the next observation is derived from that state. \n",
        "\n",
        "An example of this can be represented by speech tagging, where the observations are words and the hidden states are parts of speech.\n",
        "\n",
        "'''Dynamic programming helps us when implementing an HMM. DP is a way of making your algorithm more efficient by storing some of the intermediate results. It's useful when your algorithm has a lot of repetitive computations.'''\n",
        "\n",
        "HMM are composed by some elements:\n",
        "\n",
        "1. transition probabilities matrix (matrix of size S by S\n",
        "         stores probability from state i to state j)\n",
        "\n",
        "2. emission probability matrix ( matrix of size S by N (number of observations)\n",
        "         stores the probability of observing  O_j  from state  S_i)\n",
        "\n",
        "3.  Initial state probabilities of size S. (A starting probability distribution over states)\n",
        "\n",
        "4. Final probability: A final probability distribution over states\n",
        "\n",
        "Another perspective of HMMs is that they are an extension on mixture models that includes a transition matrix. Conceptually, a mixture model has a set of \"hidden\" states---the mixture components---and one can calculate the probability that each sample belongs to each component. This approach treats each observations independently. However, like in the part-of-speech example we know that an adjective typically is followed by a noun, and so position in the sequence matters. A HMM adds a transition matrix between the hidden states to incorporate this information across the sequence, allowing for higher probabilities of transitioning from the \"adjective\" hidden state to a noun or verb.\n"
      ],
      "metadata": {
        "id": "NaJIGx1mmoit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pomegranate implements HMMs in a flexible manner"
      ],
      "metadata": {
        "id": "DZgI_j6CqpQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pomegranate"
      ],
      "metadata": {
        "id": "5ZrNktXsqxb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install watermark"
      ],
      "metadata": {
        "id": "erYMI3c1rx4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn; seaborn.set_style('whitegrid')\n",
        "import numpy\n",
        "\n",
        "from pomegranate import *\n",
        "\n",
        "numpy.random.seed(0)\n",
        "numpy.set_printoptions(suppress=True)\n",
        "\n",
        "%load_ext watermark\n",
        "%watermark -m -n -p numpy,scipy,pomegranate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFzbr51emoQa",
        "outputId": "43c22a31-857b-491d-dc58-75fa4d43b1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy      : 1.21.6\n",
            "scipy      : 1.7.3\n",
            "pomegranate: 0.14.8\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.10.133+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CG islands identification example\n",
        "\n",
        "- CG islands are CG rich regions on a sequence of DNA. \n",
        "\n",
        "DNA is made up of the four canonical nucleotides, abbreviated 'A', 'C', 'G', and 'T'. We can say that regions of the genome that are enriched for nucleotides 'C' and 'G' are 'CG islands'.\n",
        "\n",
        "important: The issue with identifying these regions is that they are not exclusively made up of the nucleotides 'C' and 'G', but have some 'A's and 'T's scatted amongst them.\n",
        "\n",
        "A simple model that looked for long stretches of C's and G's would not perform well, because it would miss most of the real regions.\n",
        "\n",
        "We can start off by building the model. Because HMMs involve the transition matrix, which is often represented using a graph over the hidden states, building them requires a few more steps that a simple distribution or the mixture model. Our simple model will be composed of two distributions. One distribution wil be a uniform distribution across all four characters and one will have a preference for the nucleotides C and G, while still allowing the nucleotides A and T to be present."
      ],
      "metadata": {
        "id": "At_ECm6yr6qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = DiscreteDistribution({'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25})\n",
        "d2 = DiscreteDistribution({'A': 0.10, 'C': 0.40, 'G': 0.40, 'T': 0.10})"
      ],
      "metadata": {
        "id": "jk-L_J-9snh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the HMM we have to first define a set of hidden states, which are a pair of a distribution and a name."
      ],
      "metadata": {
        "id": "IlTYJVZCsrjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = State(d1, name='background')\n",
        "s2 = State(d2, name='CG island')"
      ],
      "metadata": {
        "id": "oePh2lN4s0rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the HMM and pass in the states."
      ],
      "metadata": {
        "id": "4679xY7Ts2v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HiddenMarkovModel()\n",
        "model.add_states(s1, s2)"
      ],
      "metadata": {
        "id": "JuyVwtRYs3V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to define the transition matrix, which is the probability of going from one hidden state to the next hidden state. In some cases, like this one, there are high self-loop probabilities, indicating that it's likely that one will stay in the same hidden state from one observation to the next in the sequence. Other cases have a lower probability of staying in the same state, like the part of speech tagger. A part of the transition matrix is the start probabilities, which is the probability of starting in each of the hidden states. Because we create these transitions one at a time, they are very amenable to sparse transition matrices, where it is impossible to transition from one hidden state to the next."
      ],
      "metadata": {
        "id": "6fZYv70KtAa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add_transition(model.start, s1, 0.5)\n",
        "model.add_transition(model.start, s2, 0.5)\n",
        "model.add_transition(s1, s1, 0.9)\n",
        "model.add_transition(s1, s2, 0.1)\n",
        "model.add_transition(s2, s1, 0.1)\n",
        "model.add_transition(s2, s2, 0.9)"
      ],
      "metadata": {
        "id": "2WtUAMJwtA37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, finally, we need to bake the model in order to finalize the internal structure. Bake must be called when the model has been fully specified."
      ],
      "metadata": {
        "id": "w1HzQkSDtG4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.bake()"
      ],
      "metadata": {
        "id": "9nlwr3MFtHXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can make predictions on some sequence. Let's create some sequence that has a CG enriched region in the middle and see whether we can identify it."
      ],
      "metadata": {
        "id": "5DuB7KXatKgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq = numpy.array(list('CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC'))\n",
        "\n",
        "hmm_predictions = model.predict(seq)\n",
        "\n",
        "print(\"sequence: {}\".format(''.join(seq)))\n",
        "print(\"hmm pred: {}\".format(''.join(map( str, hmm_predictions))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSFvYZdvtK3X",
        "outputId": "8c447962-caae-4524-f9a4-0fc5a3a8cc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence: CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC\n",
            "hmm pred: 111111111111111000000000000000011111111111111110000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like it successfully identified a CG island in the middle (the long stretch of 0's) and another shorter one at the end. The predicted integers don't correspond to the order in which states were added to the model, but rather, the order that they exist in the model after a topological sort. More importantly, the model wasn't tricked into thinking that every CG or even pair of CGs was an island. It required many C's and G's to be part of a longer stretch to identify that region as an island. Naturally, the balance of the transition and emission probabilities will heavily influence what regions are detected."
      ],
      "metadata": {
        "id": "mKrp8shytWL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say, though, that we want to get rid of that CG island prediction at the end because we don't believe that real islands can occur at the end of the sequence. We can take care of this by adding in an explicit end state that only the non-island hidden state can get to. We enforce that the model has to end in the end state, and if only the non-island state gets there, the sequence of hidden states must end in the non-island state. Here's how:"
      ],
      "metadata": {
        "id": "vWCP7Fn1wdAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HiddenMarkovModel()\n",
        "model.add_states(s1, s2)\n",
        "model.add_transition(model.start, s1, 0.5)\n",
        "model.add_transition(model.start, s2, 0.5)\n",
        "model.add_transition(s1, s1, 0.89 )\n",
        "model.add_transition(s1, s2, 0.10 )\n",
        "model.add_transition(s1, model.end, 0.01)\n",
        "model.add_transition(s2, s1, 0.1 )\n",
        "model.add_transition(s2, s2, 0.9)\n",
        "model.bake()"
      ],
      "metadata": {
        "id": "4NvS2ogRwdbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that all we did was add a transition from s1 to model.end with some low probability. This probability doesn't have to be high if there's only a single transition there, because there's no other possible way of getting to the end state."
      ],
      "metadata": {
        "id": "pqYiDi8AwhIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq = numpy.array(list('CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC'))\n",
        "\n",
        "hmm_predictions = model.predict(seq)\n",
        "\n",
        "print(\"sequence: {}\".format(''.join(seq)))\n",
        "print(\"hmm pred: {}\".format(''.join(map( str, hmm_predictions))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRl436viw07r",
        "outputId": "96bc46c0-23ec-4b24-b02c-4dce3e2df303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence: CGACTACTGACTACTCGCCGACGCGACTGCCGTCTATACTGCGCATACGGC\n",
            "hmm pred: 111111111111111000000000000000011111111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems far more reasonable. There is a single CG island surrounded by background sequence, and something at the end. If we knew that CG islands cannot occur at the end of sequences, we need only modify the underlying structure of the HMM in order to say that the sequence must end from the background state.\n",
        "\n",
        "In the same way that mixtures could provide probabilistic estimates of class assignments rather than only hard labels, hidden Markov models can do the same. These estimates are the posterior probabilities of belonging to each of the hidden states given the observation, but also given the rest of the sequence."
      ],
      "metadata": {
        "id": "T0Lddlrow4Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict_proba(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgtlgSMDw4wX",
        "outputId": "0082428f-06a7-451c-b6b3-3dd354d37788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4827054  0.5172946 ]\n",
            " [0.42204064 0.57795936]\n",
            " [0.28598775 0.71401225]\n",
            " [0.25756456 0.74243544]\n",
            " [0.16040038 0.83959962]\n",
            " [0.13871983 0.86128017]\n",
            " [0.17217854 0.82782146]\n",
            " [0.14750165 0.85249835]\n",
            " [0.18021186 0.81978814]\n",
            " [0.15446181 0.84553819]\n",
            " [0.18788044 0.81211956]\n",
            " [0.16252887 0.83747113]\n",
            " [0.19841088 0.80158912]\n",
            " [0.32919701 0.67080299]\n",
            " [0.38366073 0.61633927]\n",
            " [0.58044619 0.41955381]\n",
            " [0.69075524 0.30924476]\n",
            " [0.74653183 0.25346817]\n",
            " [0.76392808 0.23607192]\n",
            " [0.7479817  0.2520183 ]\n",
            " [0.69407484 0.30592516]\n",
            " [0.74761829 0.25238171]\n",
            " [0.76321595 0.23678405]\n",
            " [0.74538467 0.25461533]\n",
            " [0.68896078 0.31103922]\n",
            " [0.57760471 0.42239529]\n",
            " [0.58391467 0.41608533]\n",
            " [0.53953778 0.46046222]\n",
            " [0.6030831  0.3969169 ]\n",
            " [0.61516689 0.38483311]\n",
            " [0.57928847 0.42071153]\n",
            " [0.48505793 0.51494207]\n",
            " [0.30518744 0.69481256]\n",
            " [0.25379428 0.74620572]\n",
            " [0.12610747 0.87389253]\n",
            " [0.08105965 0.91894035]\n",
            " [0.07637934 0.92362066]\n",
            " [0.10767468 0.89232532]\n",
            " [0.20431225 0.79568775]\n",
            " [0.23273876 0.76726124]\n",
            " [0.35851961 0.64148039]\n",
            " [0.40985726 0.59014274]\n",
            " [0.40161837 0.59838163]\n",
            " [0.33141706 0.66858294]\n",
            " [0.17892403 0.82107597]\n",
            " [0.12850549 0.87149451]\n",
            " [0.13285026 0.86714974]\n",
            " [0.19603531 0.80396469]\n",
            " [0.19760431 0.80239569]\n",
            " [0.13801161 0.86198839]\n",
            " [0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see here the transition from the first non-island region to the middle island region, with high probabilities in one column turning into high probabilities in the other column. The predict method is just taking the most likely element, the maximum-a-posteriori estimate."
      ],
      "metadata": {
        "id": "3Xjhr4_9xY2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to using the forward-backward algorithm to just calculate posterior probabilities for each observation, we can count the number of transitions that are predicted to occur between the hidden states."
      ],
      "metadata": {
        "id": "wJXB3N5bxjMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans, ems = model.forward_backward(seq)\n",
        "print(trans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfowUipvxjih",
        "outputId": "0465ee7c-c6ec-494e-cf00-000efd3fdb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.78100555  2.89559314  0.          0.        ]\n",
            " [ 2.41288774 28.91051356  0.          1.        ]\n",
            " [ 0.4827054   0.5172946   0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the transition table, which has the soft count of the number of transitions across an edge in the model given a single sequence. It is a square matrix of size equal to the number of states (including start and end state), with number of transitions from (row_id) to (column_id). This is exemplified by the 1.0 in the first row, indicating that there is one transition from background state to the end state, as that's the only way to reach the end state. However, the third (or fourth, depending on ordering) row is the transitions from the start state, and it only slightly favors the background state. These counts are not normalized to the length of the input sequence, but can easily be done so by dividing by row sums, column sums, or entire table sums, depending on your application.\n",
        "\n",
        "A possible reason not to normalize is to run several sequences through and add up their tables, because normalizing in the end and extracting some domain knowledge. It is extremely useful in practice. For example, we can see that there is an expectation of ~2.9 transitions from CG island to background, and ~2.4 from background to CG island. This could be used to infer that there are ~2-3 edges, which makes sense if you consider that the start and end of the sequence seem like they might be part of the CG island states except for the strict transition probabilities used (look at the first few rows of the emission table above.)"
      ],
      "metadata": {
        "id": "o39u8h_bzrG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK10b87Uxswe",
        "outputId": "6be8c20a-48c3-4c55-df59-b76b765f17ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.72834874 -0.65914274]\n",
            " [-0.86265366 -0.54825173]\n",
            " [-1.25180628 -0.33685517]\n",
            " [-1.35648488 -0.29781936]\n",
            " [-1.83008222 -0.17483014]\n",
            " [-1.97529901 -0.14933542]\n",
            " [-1.75922331 -0.18895778]\n",
            " [-1.91391594 -0.159584  ]\n",
            " [-1.71362211 -0.19870934]\n",
            " [-1.86780839 -0.16778195]\n",
            " [-1.6719495  -0.2081077 ]\n",
            " [-1.81689961 -0.17736849]\n",
            " [-1.61741525 -0.22115912]\n",
            " [-1.11109888 -0.3992798 ]\n",
            " [-0.95799663 -0.48395771]\n",
            " [-0.54395817 -0.8685635 ]\n",
            " [-0.36996973 -1.17362222]\n",
            " [-0.29231702 -1.37251703]\n",
            " [-0.26928163 -1.44361878]\n",
            " [-0.29037676 -1.37825358]\n",
            " [-0.36517548 -1.18441479]\n",
            " [-0.29086274 -1.37681261]\n",
            " [-0.27021427 -1.44060672]\n",
            " [-0.29385485 -1.36800139]\n",
            " [-0.37257093 -1.16783627]\n",
            " [-0.54886554 -0.8618137 ]\n",
            " [-0.53800042 -0.87686492]\n",
            " [-0.61704247 -0.77552447]\n",
            " [-0.50570029 -0.92402833]\n",
            " [-0.48586168 -0.95494553]\n",
            " [-0.54595471 -0.86580787]\n",
            " [-0.72348695 -0.66370087]\n",
            " [-1.18682912 -0.36411317]\n",
            " [-1.37123124 -0.29275396]\n",
            " [-2.07062079 -0.13479788]\n",
            " [-2.51256996 -0.08453407]\n",
            " [-2.57204302 -0.07945383]\n",
            " [-2.22864081 -0.11392451]\n",
            " [-1.58810581 -0.22854845]\n",
            " [-1.45783866 -0.26492793]\n",
            " [-1.02577191 -0.44397667]\n",
            " [-0.89194632 -0.52739084]\n",
            " [-0.91225297 -0.51352655]\n",
            " [-1.10437769 -0.40259482]\n",
            " [-1.72079399 -0.19713964]\n",
            " [-2.05178363 -0.13754572]\n",
            " [-2.01853267 -0.1425436 ]\n",
            " [-1.62946046 -0.21819993]\n",
            " [-1.62148867 -0.22015342]\n",
            " [-1.98041746 -0.14851348]\n",
            " [       -inf  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "emission matrix"
      ],
      "metadata": {
        "id": "GJovnnvm0Lr0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqJYpCl70NAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}